
<!-- saved from url=(0083)https://www.cs.cityu.edu.hk/~rynson/projects/mirror_glass/MirrorGlassDetection.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="Generator" content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:SimSun;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
@font-face
	{font-family:"\@SimSun";
	panose-1:2 1 6 0 3 1 1 1 1 1;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:10.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{color:purple;
	text-decoration:underline;}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-link:"Balloon Text Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:8.0pt;
	font-family:"Tahoma",sans-serif;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:10.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	margin-bottom:.0001pt;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	margin-bottom:.0001pt;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:10.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-link:"Balloon Text";
	font-family:"Tahoma",sans-serif;}
.MsoChpDefault
	{font-family:"Calibri",sans-serif;}
.MsoPapDefault
	{margin-bottom:10.0pt;
	line-height:115%;}
 /* Page Definitions */
 @page WordSection1
	{size:11.0in 17.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>

<style class="mpa-style-fix ImageGatherer">.FotorFrame{position:fixed!important}</style><style class="mpa-style-fix SideFunctionPanel">.weui-desktop-online-faq__wrp{top:304px!important;bottom:unset!important}.weui-desktop-online-faq__wrp .weui-desktop-online-faq__switch{width:38px!important}</style></head>

<body lang="EN-US" link="blue" vlink="purple" mpa-version="7.15.3" mpa-extension-id="ibefaeehajgcpooopoegkifhgecigeeg">

<div class="WordSection1">

<div align="center">

<table class="MsoTableGrid" border="1" cellspacing="0" cellpadding="0" width="100%" style="width:100.0%;border-collapse:collapse;border:none">
 <tbody><tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;border-bottom:
  solid windowtext 1.5pt;padding:0in 5.4pt 0in 5.4pt">
  <p class="MsoNormal" align="center" style="margin-top:6.0pt;margin-right:0in;
  margin-bottom:12.0pt;margin-left:0in;text-align:center;line-height:normal"><b><span style="font-size:18.0pt;font-family:&quot;Times New Roman&quot;,serif">Mirror and Glass
  Detection/Segmentation</span></b></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;border-bottom:
  solid windowtext 1.5pt;padding:0in 5.4pt 0in 5.4pt">
  <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;line-height:
  normal"><span style="font-size:12.0pt;font-family:&quot;Times New Roman&quot;,serif">&nbsp;</span></p>
  <p class="MsoNormal" style="margin-bottom:10.0pt;text-align:justify;text-justify:
  inter-ideograph;line-height:115%"><span style="font-size:12.0pt;line-height:
  115%;font-family:&quot;Times New Roman&quot;,serif">In this project, we are developing
  techniques for mirror and glass detection/segmentation. While a mirror is a
  reflective surface that reflects the scene in front of it, glass is a transparent
  surface that transmits the scene from the back side and often also reflects
  the scene in front of it too. In general, both mirrors and glass do not have
  their own visual appearances. They only reflect/transmit the appearances of
  their surroundings.</span></p>
  <p class="MsoNormal" style="margin-bottom:10.0pt;text-align:justify;text-justify:
  inter-ideograph;line-height:115%"><span style="font-size:12.0pt;line-height:
  115%;font-family:&quot;Times New Roman&quot;,serif">As mirrors and glass do not have
  their own appearances, it is not straightforward to develop automatic
  algorithms to detect and segment them. However, as they appear everywhere in
  our daily life, it can be problematic if we are not able to detect them
  reliably. For example, a vision-based depth sensor may falsely estimate the
  depth of a piece of mirror/glass as the depth of the objects inside it, a
  robot may not be aware of the presence of a mirror/glass wall, and a drone
  may collide into a high rise (noted that most high rises are covered by glass
  these days).</span></p>
  <p class="MsoNormal" style="margin-bottom:10.0pt;text-align:justify;text-justify:
  inter-ideograph;line-height:115%"><span style="font-size:12.0pt;line-height:
  115%;font-family:&quot;Times New Roman&quot;,serif">To the best of our knowledge, my
  team is the first to develop computational models for automatic detection and
  segmentation of mirror and transparent glass surfaces. Although there have
  been some works that investigate the detection of transparent glass objects,
  these methods mainly focus on detecting wine glass and small glass objects,
  which have some special visual properties that can be used for detection. Unlike
  these works, we are more interested in detecting general glass surfaces that
  may not possess any special properties of their own.</span></p>
  <p class="MsoNormal" style="margin-bottom:10.0pt;text-align:justify;text-justify:
  inter-ideograph;line-height:115%"><span style="font-size:12.0pt;line-height:
  115%;font-family:&quot;Times New Roman&quot;,serif">We are also interested in exploring
  the application of our mirror/glass detection methods in autonomous
  navigation.</span></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;padding:0in 5.4pt 0in 5.4pt">
  <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
  0in;margin-left:.05in;margin-bottom:.0001pt;line-height:normal"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Progressive Mirror Detection</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> </span><span style="font-size:
  10.0pt;font-family:&quot;Times New Roman&quot;,serif">[</span><a href="http://www.cs.cityu.edu.hk/~rynson/papers/cvpr20c.pdf"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">paper</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] [</span><a href="http://www.cs.cityu.edu.hk/~rynson/papers/demos/cvpr20c-supp.pdf"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">suppl</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] [code]
  [dataset]</span></p>
  <p class="MsoNormal" style="margin-top:0in;margin-right:0in;margin-bottom:0in;
  margin-left:.05in;margin-bottom:.0001pt;line-height:normal"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">Jiaying Lin,
  Guodong Wang, and Rynson Lau</span></p>
  <p class="MsoNormal" style="margin-top:0in;margin-right:0in;margin-bottom:6.0pt;
  margin-left:.05in;line-height:normal"><b><i><span style="font-family:&quot;Times New Roman&quot;,serif;
  color:#C00000">Proc. IEEE CVPR</span></i></b><span style="font-family:&quot;Times New Roman&quot;,serif">,
  June 2020</span></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;padding:0in 5.4pt 0in 5.4pt">
  <div align="center">
  <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" style="border-collapse:collapse;border:none">
   <tbody><tr style="height:142.8pt">
    <td width="623" valign="top" style="width:513.45pt;padding:0in 5.4pt 0in 5.4pt;
    height:142.8pt">
    <p class="MsoNormal" align="center" style="margin-top:0in;margin-right:0in;
    margin-bottom:0in;margin-left:.05in;margin-bottom:.0001pt;text-align:center;
    line-height:normal"><img border="0" width="603" height="179" src="./MirrorGlassDetection_files/image001.jpg"></p>
    <p class="MsoNormal" style="margin-top:0in;margin-right:-2.9pt;margin-bottom:
    0in;margin-left:0in;margin-bottom:.0001pt;text-align:justify;text-justify:
    inter-ideograph;line-height:normal"><span style="font-size:9.0pt;
    font-family:&quot;Times New Roman&quot;,serif">Visualization of our progressive
    approach to recognizing mirrors from a single image. By finding
    correspondences between objects inside and outside of the mirror and then
    explicitly locating the miror edges, we can detect the mirror region more
    reliably.</span></p>
    </td>
   </tr>
  </tbody></table>
  </div>
  <p class="MsoNormal" style="margin-top:0in;margin-right:0in;margin-bottom:0in;
  margin-left:.05in;margin-bottom:.0001pt;line-height:normal;text-autospace:
  none"></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;padding:0in 5.4pt 0in 5.4pt">
  <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
  6.0pt;margin-left:.05in;text-align:justify;text-justify:inter-ideograph;
  line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Input-Output:
  </span></b><span style="font-family:&quot;Times New Roman&quot;,serif">Given an input
  image, our network outputs a binary mask that indicate where mirrors are.</span></p>
  <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
  6.0pt;margin-left:.05in;text-align:justify;text-justify:inter-ideograph;
  line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Abstract.</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> The mirror detection problem is
  important as mirrors can affect the performances of many vision tasks. It is
  a difficult problem since it requires an understanding of global scene
  semantics. Recently, a method was proposed to detect mirrors by learning
  multi-level contextual contrasts between inside and outside of mirrors, which
  helps locate mirror edges implicitly. We observe that the content of a mirror
  reflects the content of its surrounding, separated by the edge of the mirror.
  Hence, we propose a model in this paper to progressively learn the content
  similarity between the inside and outside of the mirror while explicitly
  detecting the mirror edges. Our work has two main contributions. First, we
  propose a new relational contextual contrasted local (RCCL) module to extract
  and compare the mirror features with its corresponding context features, and
  an edge detection and fusion (EDF) module to learn the features of mirror edges
  in complex scenes via explicit supervision. Second, we construct a
  challenging benchmark dataset of 6,461 mirror images. Unlike the existing MSD
  dataset, which has limited diversity, our dataset covers a variety of scenes
  and is much larger in scale. Experimental results show that our model
  outperforms relevant state-of-the-art methods.</span></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border-top:solid windowtext 1.5pt;
  border-left:none;border-bottom:solid windowtext 1.5pt;border-right:none;
  padding:0in 5.4pt 0in 5.4pt">
  <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" width="100%" style="width:100.0%;border-collapse:collapse;border:none">
   <tbody><tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    0in;margin-left:0in;margin-bottom:.0001pt;line-height:normal"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Don�t Hit Me! Glass Detection
    in Real-world Scenes</span></b><span style="font-family:&quot;Times New Roman&quot;,serif">
    </span><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">[</span><a href="http://www.cs.cityu.edu.hk/~rynson/papers/cvpr20d.pdf"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">paper</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] [</span><a href="http://www.cs.cityu.edu.hk/~rynson/papers/demos/cvpr20d-supp.pdf"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">suppl</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] [code]
    [dataset]</span></p>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">Haiyang
    Mei, Xin Yang, Yang Wang, Yuanyuan Liu, Shengfeng He, Qiang Zhang, Xiaopeng
    Wei, and Rynson Lau</span></p>
    <p class="MsoNormal" style="margin-bottom:6.0pt;line-height:normal"><b><i><span style="font-family:&quot;Times New Roman&quot;,serif;color:#C00000">Proc. IEEE CVPR</span></i></b><span style="font-family:&quot;Times New Roman&quot;,serif">, June 2020</span></p>
    </td>
   </tr>
   <tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <div align="center">
    <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" style="border-collapse:collapse;border:none">
     <tbody><tr style="height:142.8pt">
      <td width="608" valign="top" style="width:513.45pt;padding:0in 5.4pt 0in 5.4pt;
      height:142.8pt">
      <p class="MsoNormal" align="center" style="margin-bottom:0in;margin-bottom:
      .0001pt;text-align:center;line-height:normal"><img border="0" width="515" height="372" src="./MirrorGlassDetection_files/image002.jpg"></p>
      <p class="MsoNormal" style="margin-top:0in;margin-right:-2.9pt;margin-bottom:
      0in;margin-left:0in;margin-bottom:.0001pt;text-align:justify;text-justify:
      inter-ideograph;line-height:normal"><span style="font-size:9.0pt;
      font-family:&quot;Times New Roman&quot;,serif">Problems with glass in existing
      vision tasks. In depth prediction, existing method [16] wrongly predicts
      the depth of the scene behind the glass, instead of the depth to the
      glass (1st row of (b)). For instance segmentation, Mask RCNN [9] only
      segments the instances behind the glass, not aware that they are actually
      behind the glass (2nd row of (b)). Besides, if we directly apply an existing
      singe-image reflection removal (SIRR) method [36] to an image that is
      only partially covered by glass, the non-glass region can be corrupted
      (3rd row of (b)). GDNet can detect the glass (c) and then correct these
      failure cases (d).</span></p>
      </td>
     </tr>
    </tbody></table>
    </div>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal;text-autospace:none"></p>
    </td>
   </tr>
   <tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Input-Output:
    </span></b><span style="font-family:&quot;Times New Roman&quot;,serif">Given an input
    image, our network outputs a binary mask that indicate where transparent glass
    regions are.</span></p>
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Abstract.</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> Transparent glass is very
    common in our daily life. Existing computer vision systems neglect it and
    thus may have severe consequences, e.g., a robot may crash into a glass
    wall. However, sensing the presence of glass is not straightforward. The
    key challenge is that arbitrary objects/scenes can appear behind the glass,
    and the content within the glass region is typically similar to those
    behind it. In this paper, we propose an important problem of detecting
    glass from a single RGB image. To address this problem, we construct a
    large-scale glass detection dataset (GDD) and design a glass detection
    network, called GDNet, which explores abundant contextual cues for robust
    glass detection with a novel large-field contextual feature integration
    (LCFI) module. Extensive experiments demonstrate that the proposed method
    achieves more superior glass detection results on our GDD test set than
    state-of-the-art methods fine-tuned for glass detection.</span></p>
    </td>
   </tr>
  </tbody></table>
  <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;line-height:
  normal"></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;border-bottom:
  solid windowtext 1.5pt;padding:0in 5.4pt 0in 5.4pt">
  <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" width="100%" style="width:100.0%;border-collapse:collapse;border:none">
   <tbody><tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    0in;margin-left:0in;margin-bottom:.0001pt;line-height:normal"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Where is My Mirror?</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> </span><span style="font-size:
    10.0pt;font-family:&quot;Times New Roman&quot;,serif">[</span><a href="http://www.cs.cityu.edu.hk/~rynson/papers/iccv19a.pdf"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">paper</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] [</span><a href="http://www.cs.cityu.edu.hk/~rynson/papers/demos/iccv19a-supp.pdf"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">suppl</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] [</span><a href="https://github.com/Mhaiyang/ICCV2019_MirrorNet"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">code and
    updated results</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">]
    [</span><a href="https://drive.google.com/file/d/1Znw92fO6lCKfXejjSSyMyL1qtFepgjPI/view?usp=sharing"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">dataset</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] </span></p>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">Xin
    Yang*, Haiyang Mei*, Ke Xu, Xiaopeng Wei, Baocai Yin, and Rynson Lau </span><span style="font-size:9.0pt;font-family:&quot;Times New Roman&quot;,serif">(* joint first
    authors)</span><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">
    </span></p>
    <p class="MsoNormal" style="margin-bottom:6.0pt;line-height:normal"><b><i><span style="font-family:&quot;Times New Roman&quot;,serif;color:#C00000">Proc. IEEE ICCV</span></i></b><span style="font-family:&quot;Times New Roman&quot;,serif">, Oct. 2019</span></p>
    </td>
   </tr>
   <tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <div align="center">
    <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" style="border-collapse:collapse;border:none">
     <tbody><tr style="height:22.5pt">
      <td width="608" valign="top" style="width:513.45pt;padding:0in 5.4pt 0in 5.4pt;
      height:22.5pt">
      <p class="MsoNormal" align="center" style="margin-bottom:0in;margin-bottom:
      .0001pt;text-align:center;line-height:normal"><img border="0" width="449" height="460" src="./MirrorGlassDetection_files/image003.jpg"></p>
      <p class="MsoNormal" style="margin-top:0in;margin-right:-2.9pt;margin-bottom:
      0in;margin-left:0in;margin-bottom:.0001pt;text-align:justify;text-justify:
      inter-ideograph;line-height:normal"><span style="font-size:9.0pt;
      font-family:&quot;Times New Roman&quot;,serif">Problems with mirrors in existing
      vision tasks. In depth prediction, NYU-v2 dataset [32] uses a Kinect to
      capture depth as ground truth. It wrongly predicts the depths of the
      reflected contents, instead of the mirror depths (b). In instance
      semantic segmentation, Mask RCNN [12] wrongly detects objects inside the
      mirrors (c). With MirrorNet, we first detect and mask out the mirrors
      (d). We then obtain the correct depths (e), by interpolating the depths
      from surrounding pixels of the mirrors, and segmentation maps (f).</span></p>
      </td>
     </tr>
    </tbody></table>
    </div>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal;text-autospace:none"></p>
    </td>
   </tr>
   <tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Input-Output:
    </span></b><span style="font-family:&quot;Times New Roman&quot;,serif">Given an input
    image, our network outputs a binary mask that indicate where mirrors are.</span></p>
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Abstract.</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> Mirrors are everywhere in our
    daily lives. Existing computer vision systems do not consider mirrors, and
    hence may get confused by the reflected content inside a mirror, resulting
    in a severe performance degradation. However, separating the real content
    outside a mirror from the reflected content inside it is non-trivial. The
    key challenge is that mirrors typically reflect contents similar to their
    surroundings, making it very difficult to differentiate the two. In this
    paper, we present a novel method to segment mirrors from an input image. To
    the best of our knowledge, this is the first work to address the mirror
    segmentation problem with a computational approach. We make the following
    contributions. First, we construct a large-scale mirror dataset that
    contains mirror images with corresponding manually annotated masks. This
    dataset covers a variety of daily life scenes, and will be made publicly
    available for future research. Second, we propose a novel network, called
    MirrorNet, for mirror segmentation, by modeling both semantical and
    low-level color/texture discontinuities between the contents inside and
    outside of the mirrors. Third, we conduct extensive experiments to evaluate
    the proposed method, and show that it outperforms the carefully chosen
    baselines from the state-of-the-art detection and segmentation methods</span></p>
    </td>
   </tr>
  </tbody></table>
  <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;line-height:
  normal"></p>
  </td>
 </tr>
</tbody></table>
</div>
<img src="./MirrorGlassDetection_files/counter.cgi" width="1" height="1">
<p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:0in;
margin-left:0in;margin-bottom:.0001pt"><i><span style="font-size:9.0pt;
line-height:115%;font-family:&quot;Times New Roman&quot;,serif">Last updated in July
2020.</span></i></p>

</div>




<div class="mpa-sc mpa-plugin-article-gatherer mpa-new mpa-rootsc" data-z="100" style="display: block;" id="mpa-rootsc-article-gatherer"></div><div class="mpa-sc mpa-plugin-image-gatherer mpa-new mpa-rootsc" data-z="100" style="display: block;" id="mpa-rootsc-image-gatherer"></div><div class="mpa-sc mpa-plugin-page-clipper mpa-new mpa-rootsc" data-z="100" style="display: block;" id="mpa-rootsc-page-clipper"></div><div class="mpa-sc mpa-plugin-text-gatherer mpa-new mpa-rootsc" data-z="100" style="display: block;" id="mpa-rootsc-text-gatherer"></div><div class="mpa-sc mpa-plugin-video-gatherer mpa-new mpa-rootsc" data-z="100" style="display: block;" id="mpa-rootsc-video-gatherer"></div><div class="mpa-sc mpa-plugin-side-function-panel mpa-new mpa-rootsc" data-z="110" style="display: block;" id="mpa-rootsc-side-function-panel"></div><div class="mpa-sc mpa-plugin-notifier mpa-new mpa-rootsc" data-z="120" style="display: block;" id="mpa-rootsc-notifier"></div><div class="mpa-sc mpa-plugin-notification-manager mpa-new mpa-rootsc" data-z="130" style="display: block;" id="mpa-rootsc-notification-manager"></div></body></html>